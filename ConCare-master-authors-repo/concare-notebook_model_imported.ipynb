{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:20.230506Z",
     "start_time": "2021-11-02T02:11:17.941835Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import importlib\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:20.237620Z",
     "start_time": "2021-11-02T02:11:20.234044Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "file_name = './model/concare0'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:20.399479Z",
     "start_time": "2021-11-02T02:11:20.239796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build readers, discretizers, normalizers\n",
    "train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'train'),\n",
    "                                         listfile=os.path.join(data_path, 'train_listfile.csv'),\n",
    "                                         period_length=48.0)\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'train'),\n",
    "                                       listfile=os.path.join(data_path, 'val_listfile.csv'),\n",
    "                                       period_length=48.0)\n",
    "\n",
    "discretizer = Discretizer(timestep=arg_timestep,\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:11:20.411518Z",
     "start_time": "2021-11-02T02:11:20.401666Z"
    }
   },
   "outputs": [],
   "source": [
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = 'ihm_normalizer'\n",
    "normalizer_state = os.path.join(os.path.dirname(data_path), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:12:44.729970Z",
     "start_time": "2021-11-02T02:11:20.413422Z"
    }
   },
   "outputs": [],
   "source": [
    "n_trained_chunks = 0\n",
    "train_raw = utils.load_data(train_reader, discretizer, normalizer, small_part, return_names=True)\n",
    "val_raw = utils.load_data(val_reader, discretizer, normalizer, small_part, return_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:12:48.064359Z",
     "start_time": "2021-11-02T02:12:44.733279Z"
    }
   },
   "outputs": [],
   "source": [
    "demographic_data = []\n",
    "diagnosis_data = []\n",
    "idx_list = []\n",
    "\n",
    "demo_path = data_path + 'demographic/'\n",
    "for cur_name in os.listdir(demo_path):\n",
    "    cur_id, cur_episode = cur_name.split('_', 1)\n",
    "    cur_episode = cur_episode[:-4]\n",
    "    cur_file = demo_path + cur_name\n",
    "\n",
    "    with open(cur_file, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        if header[0] != \"Icustay\":\n",
    "            continue\n",
    "        cur_data = tsfile.readline().strip().split(',')\n",
    "        \n",
    "    if len(cur_data) == 1:\n",
    "        cur_demo = np.zeros(12)\n",
    "        cur_diag = np.zeros(128)\n",
    "    else:\n",
    "        if cur_data[3] == '':\n",
    "            cur_data[3] = 60.0\n",
    "        if cur_data[4] == '':\n",
    "            cur_data[4] = 160\n",
    "        if cur_data[5] == '':\n",
    "            cur_data[5] = 60\n",
    "\n",
    "        cur_demo = np.zeros(12)\n",
    "        cur_demo[int(cur_data[1])] = 1\n",
    "        cur_demo[5 + int(cur_data[2])] = 1\n",
    "        cur_demo[9:] = cur_data[3:6]\n",
    "        cur_diag = np.array(cur_data[8:], dtype=int)\n",
    "\n",
    "    demographic_data.append(cur_demo)\n",
    "    diagnosis_data.append(cur_diag)\n",
    "    idx_list.append(cur_id+'_'+cur_episode)\n",
    "\n",
    "for each_idx in range(9,12):\n",
    "    cur_val = []\n",
    "    for i in range(len(demographic_data)):\n",
    "        cur_val.append(demographic_data[i][each_idx])\n",
    "    cur_val = np.array(cur_val)\n",
    "    _mean = np.mean(cur_val)\n",
    "    _std = np.std(cur_val)\n",
    "    _std = _std if _std > 1e-7 else 1e-7\n",
    "    for i in range(len(demographic_data)):\n",
    "        demographic_data[i][each_idx] = (demographic_data[i][each_idx] - _mean) / _std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:12:48.282265Z",
     "start_time": "2021-11-02T02:12:48.191901Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:12:48.355893Z",
     "start_time": "2021-11-02T02:12:48.284802Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y, name):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.name = name\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        return self.x[index], self.y[index], self.name[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T02:12:48.442051Z",
     "start_time": "2021-11-02T02:12:48.358759Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_raw['data'][0], train_raw['data'][1], train_raw['names'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = Dataset(val_raw['data'][0], val_raw['data'][1], val_raw['names'])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T03:27:01.575443Z",
     "start_time": "2021-11-02T02:12:48.445259Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0 Batch 0: Train Loss = 0.9405\n",
      "Model Loss = 0.7100, Decov Loss = 0.0003\n",
      "Epoch 0 Batch 30: Train Loss = 0.5352\n",
      "Model Loss = 0.4974, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3941\n",
      "valid_model Loss = 0.3941\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7589779665035532\n",
      "AUC of PRC = 0.3586355611255153\n",
      "min(+P, Se) = 0.3795454545454545\n",
      "f1_score = 0.0\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 1 Batch 0: Train Loss = 0.4247\n",
      "Model Loss = 0.4205, Decov Loss = 0.0000\n",
      "Epoch 1 Batch 30: Train Loss = 0.3975\n",
      "Model Loss = 0.3941, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3898\n",
      "valid_model Loss = 0.3898\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7786343249669053\n",
      "AUC of PRC = 0.3953648278027359\n",
      "min(+P, Se) = 0.4105504587155963\n",
      "f1_score = 0.0\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 2 Batch 0: Train Loss = 0.4202\n",
      "Model Loss = 0.4178, Decov Loss = 0.0000\n",
      "Epoch 2 Batch 30: Train Loss = 0.3893\n",
      "Model Loss = 0.3868, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3344\n",
      "valid_model Loss = 0.3344\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2747   39]\n",
      " [ 383   53]]\n",
      "accuracy = 0.8690254500310366\n",
      "precision class 0 = 0.8776357827476038\n",
      "precision class 1 = 0.5760869565217391\n",
      "recall class 0 = 0.9860014357501795\n",
      "recall class 1 = 0.12155963302752294\n",
      "AUC of ROC = 0.7791850800529515\n",
      "AUC of PRC = 0.3872604383519769\n",
      "min(+P, Se) = 0.42660550458715596\n",
      "f1_score = 0.20075757575757577\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 3 Batch 0: Train Loss = 0.3385\n",
      "Model Loss = 0.3366, Decov Loss = 0.0000\n",
      "Epoch 3 Batch 30: Train Loss = 0.3651\n",
      "Model Loss = 0.3630, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3329\n",
      "valid_model Loss = 0.3329\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7972183986775292\n",
      "AUC of PRC = 0.38844323439786055\n",
      "min(+P, Se) = 0.43577981651376146\n",
      "f1_score = 0.0\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 4 Batch 0: Train Loss = 0.2883\n",
      "Model Loss = 0.2864, Decov Loss = 0.0000\n",
      "Epoch 4 Batch 30: Train Loss = 0.3483\n",
      "Model Loss = 0.3465, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3271\n",
      "valid_model Loss = 0.3271\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.8141658489037586\n",
      "AUC of PRC = 0.4439482495021904\n",
      "min(+P, Se) = 0.46559633027522934\n",
      "f1_score = 0.0\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 5 Batch 0: Train Loss = 0.3348\n",
      "Model Loss = 0.3329, Decov Loss = 0.0000\n",
      "Epoch 5 Batch 30: Train Loss = 0.3426\n",
      "Model Loss = 0.3410, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3174\n",
      "valid_model Loss = 0.3174\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.8160478012605623\n",
      "AUC of PRC = 0.44737152818396325\n",
      "min(+P, Se) = 0.44724770642201833\n",
      "f1_score = 0.0\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 6 Batch 0: Train Loss = 0.3422\n",
      "Model Loss = 0.3408, Decov Loss = 0.0000\n",
      "Epoch 6 Batch 30: Train Loss = 0.3403\n",
      "Model Loss = 0.3389, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3200\n",
      "valid_model Loss = 0.3200\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2786    0]\n",
      " [ 436    0]]\n",
      "accuracy = 0.8646803227808815\n",
      "precision class 0 = 0.8646803227808815\n",
      "precision class 1 = 0.0\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.8082516119259469\n",
      "AUC of PRC = 0.4141183425635757\n",
      "min(+P, Se) = 0.45642201834862384\n",
      "f1_score = 0.0\n",
      "\n",
      "Epoch 7 Batch 0: Train Loss = 0.3187\n",
      "Model Loss = 0.3174, Decov Loss = 0.0000\n",
      "Epoch 7 Batch 30: Train Loss = 0.3331\n",
      "Model Loss = 0.3317, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3144\n",
      "valid_model Loss = 0.3144\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2757   29]\n",
      " [ 384   52]]\n",
      "accuracy = 0.8718187461204221\n",
      "precision class 0 = 0.87774594078319\n",
      "precision class 1 = 0.6419753086419753\n",
      "recall class 0 = 0.9895908111988514\n",
      "recall class 1 = 0.11926605504587157\n",
      "AUC of ROC = 0.8190329102919578\n",
      "AUC of PRC = 0.4370168485101785\n",
      "min(+P, Se) = 0.46741573033707867\n",
      "f1_score = 0.20116054158607352\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 8 Batch 0: Train Loss = 0.3458\n",
      "Model Loss = 0.3443, Decov Loss = 0.0000\n",
      "Epoch 8 Batch 30: Train Loss = 0.3238\n",
      "Model Loss = 0.3225, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3138\n",
      "valid_model Loss = 0.3138\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2769   17]\n",
      " [ 414   22]]\n",
      "accuracy = 0.8662321539416511\n",
      "precision class 0 = 0.8699340245051838\n",
      "precision class 1 = 0.5641025641025641\n",
      "recall class 0 = 0.9938980617372577\n",
      "recall class 1 = 0.05045871559633028\n",
      "AUC of ROC = 0.8194132523693172\n",
      "AUC of PRC = 0.4346102669877515\n",
      "min(+P, Se) = 0.4584269662921348\n",
      "f1_score = 0.09263157894736843\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 9 Batch 0: Train Loss = 0.3342\n",
      "Model Loss = 0.3329, Decov Loss = 0.0000\n",
      "Epoch 9 Batch 30: Train Loss = 0.3277\n",
      "Model Loss = 0.3263, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3160\n",
      "valid_model Loss = 0.3160\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2734   52]\n",
      " [ 358   78]]\n",
      "accuracy = 0.8727498448168839\n",
      "precision class 0 = 0.8842173350582148\n",
      "precision class 1 = 0.6\n",
      "recall class 0 = 0.9813352476669059\n",
      "recall class 1 = 0.17889908256880735\n",
      "AUC of ROC = 0.821032587577468\n",
      "AUC of PRC = 0.42323422319419113\n",
      "min(+P, Se) = 0.4608501118568233\n",
      "f1_score = 0.2756183745583039\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 10 Batch 0: Train Loss = 0.2841\n",
      "Model Loss = 0.2824, Decov Loss = 0.0000\n",
      "Epoch 10 Batch 30: Train Loss = 0.3276\n",
      "Model Loss = 0.3262, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3101\n",
      "valid_model Loss = 0.3101\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2757   29]\n",
      " [ 396   40]]\n",
      "accuracy = 0.8680943513345748\n",
      "precision class 0 = 0.8744053282588011\n",
      "precision class 1 = 0.5797101449275363\n",
      "recall class 0 = 0.9895908111988514\n",
      "recall class 1 = 0.09174311926605505\n",
      "AUC of ROC = 0.8224815097769318\n",
      "AUC of PRC = 0.4246465436814968\n",
      "min(+P, Se) = 0.45642201834862384\n",
      "f1_score = 0.15841584158415842\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 11 Batch 0: Train Loss = 0.3033\n",
      "Model Loss = 0.3017, Decov Loss = 0.0000\n",
      "Epoch 11 Batch 30: Train Loss = 0.3221\n",
      "Model Loss = 0.3208, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3081\n",
      "valid_model Loss = 0.3081\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2678  108]\n",
      " [ 311  125]]\n",
      "accuracy = 0.8699565487274985\n",
      "precision class 0 = 0.8959518233522917\n",
      "precision class 1 = 0.5364806866952789\n",
      "recall class 0 = 0.9612347451543432\n",
      "recall class 1 = 0.286697247706422\n",
      "AUC of ROC = 0.8274160777676061\n",
      "AUC of PRC = 0.43756419400078733\n",
      "min(+P, Se) = 0.45662100456621\n",
      "f1_score = 0.37369207772795215\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 12 Batch 0: Train Loss = 0.2973\n",
      "Model Loss = 0.2962, Decov Loss = 0.0000\n",
      "Epoch 12 Batch 30: Train Loss = 0.3098\n",
      "Model Loss = 0.3085, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3086\n",
      "valid_model Loss = 0.3086\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2698   88]\n",
      " [ 322  114]]\n",
      "accuracy = 0.8727498448168839\n",
      "precision class 0 = 0.8933774834437086\n",
      "precision class 1 = 0.5643564356435643\n",
      "recall class 0 = 0.968413496051687\n",
      "recall class 1 = 0.26146788990825687\n",
      "AUC of ROC = 0.8303526149752695\n",
      "AUC of PRC = 0.4399575067640416\n",
      "min(+P, Se) = 0.4668192219679634\n",
      "f1_score = 0.3573667711598746\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 13 Batch 0: Train Loss = 0.2925\n",
      "Model Loss = 0.2911, Decov Loss = 0.0000\n",
      "Epoch 13 Batch 30: Train Loss = 0.3203\n",
      "Model Loss = 0.3190, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3073\n",
      "valid_model Loss = 0.3073\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2709   77]\n",
      " [ 344   92]]\n",
      "accuracy = 0.8693358162631906\n",
      "precision class 0 = 0.8873239436619719\n",
      "precision class 1 = 0.5443786982248521\n",
      "recall class 0 = 0.9723618090452262\n",
      "recall class 1 = 0.21100917431192662\n",
      "AUC of ROC = 0.8281232505910944\n",
      "AUC of PRC = 0.42542738381539624\n",
      "min(+P, Se) = 0.45454545454545453\n",
      "f1_score = 0.3041322314049587\n",
      "\n",
      "Epoch 14 Batch 0: Train Loss = 0.3274\n",
      "Model Loss = 0.3261, Decov Loss = 0.0000\n",
      "Epoch 14 Batch 30: Train Loss = 0.3150\n",
      "Model Loss = 0.3127, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3001\n",
      "valid_model Loss = 0.3001\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2761   25]\n",
      " [ 403   33]]\n",
      "accuracy = 0.867163252638113\n",
      "precision class 0 = 0.872629582806574\n",
      "precision class 1 = 0.5689655172413793\n",
      "recall class 0 = 0.9910265613783201\n",
      "recall class 1 = 0.07568807339449542\n",
      "AUC of ROC = 0.8368925229028498\n",
      "AUC of PRC = 0.4456354469085144\n",
      "min(+P, Se) = 0.4772727272727273\n",
      "f1_score = 0.13360323886639675\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 15 Batch 0: Train Loss = 0.2986\n",
      "Model Loss = 0.2971, Decov Loss = 0.0000\n",
      "Epoch 15 Batch 30: Train Loss = 0.3065\n",
      "Model Loss = 0.3047, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3090\n",
      "valid_model Loss = 0.3090\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2725   61]\n",
      " [ 346   90]]\n",
      "accuracy = 0.8736809435133458\n",
      "precision class 0 = 0.8873331162487789\n",
      "precision class 1 = 0.5960264900662252\n",
      "recall class 0 = 0.9781048097631012\n",
      "recall class 1 = 0.20642201834862386\n",
      "AUC of ROC = 0.8449019343111362\n",
      "AUC of PRC = 0.472602019365031\n",
      "min(+P, Se) = 0.4885844748858447\n",
      "f1_score = 0.30664395229982966\n",
      "\n",
      "\n",
      "------------ Save best model ------------\n",
      "\n",
      "Epoch 16 Batch 0: Train Loss = 0.3463\n",
      "Model Loss = 0.3439, Decov Loss = 0.0000\n",
      "Epoch 16 Batch 30: Train Loss = 0.3119\n",
      "Model Loss = 0.3100, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.2999\n",
      "valid_model Loss = 0.2999\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2709   77]\n",
      " [ 326  110]]\n",
      "accuracy = 0.8749224084419616\n",
      "precision class 0 = 0.8925864909390445\n",
      "precision class 1 = 0.5882352941176471\n",
      "recall class 0 = 0.9723618090452262\n",
      "recall class 1 = 0.25229357798165136\n",
      "AUC of ROC = 0.8413586609324473\n",
      "AUC of PRC = 0.4634835130007429\n",
      "min(+P, Se) = 0.4908256880733945\n",
      "f1_score = 0.3531300160513643\n",
      "\n",
      "Epoch 17 Batch 0: Train Loss = 0.2915\n",
      "Model Loss = 0.2896, Decov Loss = 0.0000\n",
      "Epoch 17 Batch 30: Train Loss = 0.3127\n",
      "Model Loss = 0.3106, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.2976\n",
      "valid_model Loss = 0.2976\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2690   96]\n",
      " [ 316  120]]\n",
      "accuracy = 0.8721291123525761\n",
      "precision class 0 = 0.8948769128409847\n",
      "precision class 1 = 0.5555555555555556\n",
      "recall class 0 = 0.9655419956927495\n",
      "recall class 1 = 0.27522935779816515\n",
      "AUC of ROC = 0.8404539078090321\n",
      "AUC of PRC = 0.4586368555034357\n",
      "min(+P, Se) = 0.481651376146789\n",
      "f1_score = 0.3680981595092025\n",
      "\n",
      "Epoch 18 Batch 0: Train Loss = 0.2844\n",
      "Model Loss = 0.2829, Decov Loss = 0.0000\n",
      "Epoch 18 Batch 30: Train Loss = 0.3115\n",
      "Model Loss = 0.3095, Decov Loss = 0.0000\n",
      "\n",
      "==>Predicting on validation\n",
      "Valid Loss = 0.3194\n",
      "valid_model Loss = 0.3194\n",
      "valid_decov Loss = 0.0000\n",
      "confusion matrix:\n",
      "[[2714   72]\n",
      " [ 332  104]]\n",
      "accuracy = 0.8746120422098076\n",
      "precision class 0 = 0.8910045961917269\n",
      "precision class 1 = 0.5909090909090909\n",
      "recall class 0 = 0.9741564967695621\n",
      "recall class 1 = 0.23853211009174313\n",
      "AUC of ROC = 0.841778519069792\n",
      "AUC of PRC = 0.4692598908196092\n",
      "min(+P, Se) = 0.4897025171624714\n",
      "f1_score = 0.33986928104575165\n",
      "\n",
      "Epoch 19 Batch 0: Train Loss = 0.2923\n",
      "Model Loss = 0.2904, Decov Loss = 0.0000\n",
      "Epoch 19 Batch 30: Train Loss = 0.3102\n",
      "Model Loss = 0.3087, Decov Loss = 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m model_batch_loss\u001b[38;5;241m.\u001b[39mappend(model_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     57\u001b[0m decov_batch_loss\u001b[38;5;241m.\u001b[39mappend(decov_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\concare_env\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\concare_env\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\concare_env\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concare_model_extracted import ConCare\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  # numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)  # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED)  # gpu\n",
    "torch.backends.cudnn.deterministic = True  # cudnn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = ConCare(input_dim=76, hidden_dim=64, d_model=64, MHD_num_head=4, d_ff=256, output_dim=1).to(device)\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "max_roc = 0\n",
    "max_prc = 0\n",
    "train_loss = []\n",
    "train_model_loss = []\n",
    "train_decov_loss = []\n",
    "valid_loss = []\n",
    "valid_model_loss = []\n",
    "valid_decov_loss = []\n",
    "history = []\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Build once for O(1) demographic lookup\n",
    "demo_map = {idx_list[i]: torch.tensor(demographic_data[i], dtype=torch.float32) for i in range(len(idx_list))}\n",
    "\n",
    "for each_epoch in range(100):\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "    decov_batch_loss = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, (batch_x, batch_y, batch_name) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        # Fast demographic batch build\n",
    "        batch_demo = [demo_map[f\"{batch_name[i].split('_', 2)[0]}_{batch_name[i].split('_', 2)[1]}\"] for i in range(len(batch_name))]\n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "        output, decov_loss = model(batch_x, batch_demo)\n",
    "\n",
    "        model_loss = get_loss(output, batch_y.unsqueeze(-1))\n",
    "        loss = model_loss + 800 * decov_loss\n",
    "\n",
    "        batch_loss.append(loss.item())\n",
    "        model_batch_loss.append(model_loss.item())\n",
    "        decov_batch_loss.append(decov_loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 30 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f' % (each_epoch, step, np.mean(np.array(batch_loss))))\n",
    "            print('Model Loss = %.4f, Decov Loss = %.4f' % (np.mean(np.array(model_batch_loss)), np.mean(np.array(decov_batch_loss))))\n",
    "    train_loss.append(np.mean(np.array(batch_loss)))\n",
    "    train_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "    train_decov_loss.append(np.mean(np.array(decov_batch_loss)))\n",
    "\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "    decov_batch_loss = []\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for step, (batch_x, batch_y, batch_name) in enumerate(valid_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "\n",
    "            # Fast demographic batch build\n",
    "            batch_demo = [demo_map[f\"{batch_name[i].split('_', 2)[0]}_{batch_name[i].split('_', 2)[1]}\"] for i in range(len(batch_name))]\n",
    "            batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "            output, decov_loss = model(batch_x, batch_demo)\n",
    "\n",
    "            model_loss = get_loss(output, batch_y.unsqueeze(-1))\n",
    "\n",
    "            loss = model_loss + 10 * decov_loss\n",
    "            batch_loss.append(loss.item())\n",
    "            model_batch_loss.append(model_loss.item())\n",
    "            decov_batch_loss.append(decov_loss.item())\n",
    "            y_pred += list(output.cpu().detach().numpy().flatten())\n",
    "            y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "    valid_loss.append(np.mean(np.array(batch_loss)))\n",
    "    valid_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "    valid_decov_loss.append(np.mean(np.array(decov_batch_loss)))\n",
    "\n",
    "    print(\"\\n==>Predicting on validation\")\n",
    "    print('Valid Loss = %.4f' % (valid_loss[-1]))\n",
    "    print('valid_model Loss = %.4f' % (valid_model_loss[-1]))\n",
    "    print('valid_decov Loss = %.4f' % (valid_decov_loss[-1]))\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "    ret = metrics.print_metrics_binary(y_true, y_pred)\n",
    "    history.append(ret)\n",
    "    print()\n",
    "\n",
    "    cur_auroc = ret['auroc']\n",
    "\n",
    "    if cur_auroc > max_roc:\n",
    "        max_roc = cur_auroc\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': each_epoch\n",
    "        }\n",
    "        # Ensure parent directory exists before saving\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "        torch.save(state, file_name)\n",
    "        print('\\n------------ Save best model ------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T03:27:16.741911Z",
     "start_time": "2021-11-02T03:27:01.578022Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(file_name)\n",
    "save_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'test'),\n",
    "                                            listfile=os.path.join(data_path, 'test_listfile.csv'),\n",
    "                                            period_length=48.0)\n",
    "test_raw = utils.load_data(test_reader, discretizer, normalizer, small_part, return_names=True)\n",
    "test_dataset = Dataset(test_raw['data'][0], test_raw['data'][1], test_raw['names'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T03:27:20.964397Z",
     "start_time": "2021-11-02T03:27:16.745558Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_name) in enumerate(test_loader):\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        # O(1) demographic lookup using the same key scheme as train/valid\n",
    "        batch_demo = [demo_map[f\"{bn.split('_', 2)[0]}_{bn.split('_', 2)[1]}\"] for bn in batch_name]\n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "        # If model returns (logits, decov_loss)\n",
    "        output, _ = model(batch_x, batch_demo)\n",
    "\n",
    "        loss = get_loss(output, batch_y.unsqueeze(-1))\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # If get_loss uses BCEWithLogitsLoss, convert logits -> probs for metrics\n",
    "        probs = output.detach().cpu().numpy().flatten()\n",
    "        y_pred.extend(probs)\n",
    "        y_true.extend(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f' % (np.mean(np.array(batch_loss))))\n",
    "\n",
    "y_pred = np.array(y_pred)                                   # positive-class probs in [0, 1]\n",
    "y_pred_prob = np.stack([1.0 - y_pred, y_pred], axis=1)      # shape (N, 2) = [P0, P1]\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred_prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T03:27:35.542743Z",
     "start_time": "2021-11-02T03:27:20.967136Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrap evaluation\n",
    "y_true_arr = np.asarray(y_true, dtype=int)\n",
    "y_pred_arr = np.asarray(y_pred_prob, dtype=float)   # 1D positive-class probabilities\n",
    "\n",
    "N = len(y_true_arr)\n",
    "N_idx = np.arange(N)\n",
    "K = 1000\n",
    "\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "for i in range(K):\n",
    "    boot_idx = np.random.choice(N_idx, N, replace=True)\n",
    "    boot_true = y_true_arr[boot_idx]\n",
    "    boot_pred = y_pred_arr[boot_idx, :]  # 1D slice is correct\n",
    "\n",
    "    test_ret = metrics.print_metrics_binary(boot_true, boot_pred, verbose=0)\n",
    "    auroc.append(test_ret[\"auroc\"])\n",
    "    auprc.append(test_ret[\"auprc\"])\n",
    "    minpse.append(test_ret[\"minpse\"])\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"{i+1}/{K}\")\n",
    "\n",
    "print(\"auroc %.4f(%.4f)\" % (np.mean(auroc), np.std(auroc)))\n",
    "print(\"auprc %.4f(%.4f)\" % (np.mean(auprc), np.std(auprc)))\n",
    "print(\"minpse %.4f(%.4f)\" % (np.mean(minpse), np.std(minpse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
